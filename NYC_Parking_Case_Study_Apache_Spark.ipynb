{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `NYC Parking Group Case Study -- Apache Spark`\n",
    "\n",
    "##### The purpose of this case study is to conduct an exploratory data analysis that will help us understand the data that NYC Police Department has collected for parking tickets. For the scope of this analysis, we will analyse the parking tickets over the year 2017. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'Sri Harsha Ravi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a PySpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class pyspark.sql.SparkSession, The entry point to programming Spark with the Dataset and DataFrame API.\n",
    "#A SparkSession can be used create DataFrame and perform several actions on it.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Group Case Study - NYC Parking\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-30-0-31-158.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Group Case Study - NYC Parking</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8868d50940>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Parking Ticket data into a PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Summons Number: bigint, Plate ID: string, Registration State: string, Issue Date: timestamp, Violation Code: int, Vehicle Body Type: string, Vehicle Make: string, Violation Precinct: int, Issuer Precinct: int, Violation Time: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing data from a csv file located in the HDFS into a PySpark dataframe\n",
    "parkingDF = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv\")\n",
    "# cache dataframe for better performance\n",
    "parkingDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons Number: long (nullable = true)\n",
      " |-- Plate ID: string (nullable = true)\n",
      " |-- Registration State: string (nullable = true)\n",
      " |-- Issue Date: timestamp (nullable = true)\n",
      " |-- Violation Code: integer (nullable = true)\n",
      " |-- Vehicle Body Type: string (nullable = true)\n",
      " |-- Vehicle Make: string (nullable = true)\n",
      " |-- Violation Precinct: integer (nullable = true)\n",
      " |-- Issuer Precinct: integer (nullable = true)\n",
      " |-- Violation Time: string (nullable = true)\n",
      "\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0143A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0400P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23 00:00:00|             5|             SUBN|        FORD|                 0|              0|         0233P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14 00:00:00|            47|             REFG|       MITSU|                14|             14|         1120A|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21 00:00:00|            69|             DELV|       INTER|                13|             13|         0555P|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printSchema returns schema in tree format\n",
    "parkingDF.printSchema()\n",
    "\n",
    "# Displaying the first 5 rows of the dataframe\n",
    "parkingDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find the total number of tickets for the year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering data to select only where tickets related to 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For the scope of this analysis, we will analyse the parking tickets over the year 2017. So we will filter the dataframe for tickets belonging to only 2017.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+--------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|Issue_Yr|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+--------+\n",
      "|    8478629828| 66623ME|                NY|2017-06-14 00:00:00|            47|             REFG|       MITSU|                14|             14|         1120A|    2017|\n",
      "|    5096917368| FZD8593|                NY|2017-06-13 00:00:00|             7|             SUBN|       ME/BE|                 0|              0|         0852P|    2017|\n",
      "|    1407740258| 2513JMG|                NY|2017-01-11 00:00:00|            78|             DELV|       FRUEH|               106|            106|         0015A|    2017|\n",
      "|    1413656420|T672371C|                NY|2017-02-04 00:00:00|            40|             TAXI|       TOYOT|                73|             73|         0525A|    2017|\n",
      "|    8480309064| 51771JW|                NY|2017-01-26 00:00:00|            64|              VAN|       INTER|                17|             17|         0256P|    2017|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we will also register this as a temporary view so that we can query it with SQL and show off basic transformations in SQL\n",
    "parkingDF.registerTempTable(\"parking_2017\")\n",
    "\n",
    "# Creating a new column called Issue_Yr by taking year part from 'Issue Date'\n",
    "parking2017DF = spark.sql(\"SELECT *, substr(`Issue Date`, 1,4) as Issue_Yr FROM parking_2017 where substr(`Issue Date`, 1,4) = '2017'\")\n",
    "parking2017DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|total_tickets_before_filter|\n",
      "+---------------------------+\n",
      "|                   10803028|\n",
      "+---------------------------+\n",
      "\n",
      "+---------------------+\n",
      "|total_tickets_in_2017|\n",
      "+---------------------+\n",
      "|              5431918|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing total no of tickets before and after filtering for Issue Yr = 2017\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import countDistinct\n",
    "parkingDF.select(countDistinct(\"Summons Number\").alias(\"total_tickets_before_filter\")).show()\n",
    "parking2017DF.select(countDistinct(\"Summons Number\").alias(\"total_tickets_in_2017\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total no of tickets in the dataframe = `10803028`\n",
    "* Total no of tickets in the dataframe for the year 2017 = `5431918`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find out the number of unique states from where the cars that got parking tickets came."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Displaying the top States with cars that got highest parking tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o540.showString.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(Registration State#811, 200)\n+- *(1) HashAggregate(keys=[Registration State#811], functions=[partial_count(Summons Number#10L)], output=[Registration State#811, count#3482L])\n   +- *(1) Project [Summons Number#10L, CASE WHEN (cast(Registration State#12 as int) = 99) THEN NY ELSE Registration State#12 END AS Registration State#811]\n      +- *(1) Filter (((((isnotnull(Issue Date#13) && isnotnull(Violation Time#19)) && (substring(cast(Issue Date#13 as string), 1, 4) = 2017)) && NOT (Violation Time#19 = nan)) && NOT (Violation Time#19 = Nan)) && NOT (CASE WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 0) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 400)) THEN Dawn WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 400) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 800)) THEN Early Mornig WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 800) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 1200)) THEN Morning Rush WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 1200) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 1600)) THEN Afternoon WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 1600) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 2000)) THEN Office Time WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 2000) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) <= 2400)) THEN Late Night ELSE unknown END = unknown))\n         +- InMemoryTableScan [Issue Date#13, Registration State#12, Summons Number#10L, Violation Time#19], [isnotnull(Issue Date#13), isnotnull(Violation Time#19), (substring(cast(Issue Date#13 as string), 1, 4) = 2017), NOT (Violation Time#19 = nan), NOT (Violation Time#19 = Nan), NOT (CASE WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 0) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 400)) THEN Dawn WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 400) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 800)) THEN Early Mornig WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 800) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 1200)) THEN Morning Rush WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 1200) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 1600)) THEN Afternoon WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 1600) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 2000)) THEN Office Time WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 2000) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) <= 2400)) THEN Late Night ELSE unknown END = unknown)]\n               +- InMemoryRelation [Summons Number#10L, Plate ID#11, Registration State#12, Issue Date#13, Violation Code#14, Vehicle Body Type#15, Vehicle Make#16, Violation Precinct#17, Issuer Precinct#18, Violation Time#19], StorageLevel(disk, memory, deserialized, 1 replicas)\n                     +- *(1) FileScan csv [Summons Number#10L,Plate ID#11,Registration State#12,Issue Date#13,Violation Code#14,Vehicle Body Type#15,Vehicle Make#16,Violation Precinct#17,Issuer Precinct#18,Violation Time#19] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/common_folder/nyc_parking/Parking_Violations_Issued_-_Fisca..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Summons Number:bigint,Plate ID:string,Registration State:string,Issue Date:timestamp,Viola...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:136)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.GeneratedMethodAccessor70.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.needToCopyObjectsBeforeShuffle(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:303)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 39 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-e9b5ef6487e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mparking2017DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Registration State\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count(`Summons Number`)\"\u001b[0m\u001b[0;34m)\u001b[0m                                                \u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total_tickets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total_tickets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark2.4/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark2.4/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o540.showString.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(Registration State#811, 200)\n+- *(1) HashAggregate(keys=[Registration State#811], functions=[partial_count(Summons Number#10L)], output=[Registration State#811, count#3482L])\n   +- *(1) Project [Summons Number#10L, CASE WHEN (cast(Registration State#12 as int) = 99) THEN NY ELSE Registration State#12 END AS Registration State#811]\n      +- *(1) Filter (((((isnotnull(Issue Date#13) && isnotnull(Violation Time#19)) && (substring(cast(Issue Date#13 as string), 1, 4) = 2017)) && NOT (Violation Time#19 = nan)) && NOT (Violation Time#19 = Nan)) && NOT (CASE WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 0) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 400)) THEN Dawn WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 400) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 800)) THEN Early Mornig WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 800) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 1200)) THEN Morning Rush WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 1200) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 1600)) THEN Afternoon WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 1600) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 2000)) THEN Office Time WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 2000) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) <= 2400)) THEN Late Night ELSE unknown END = unknown))\n         +- InMemoryTableScan [Issue Date#13, Registration State#12, Summons Number#10L, Violation Time#19], [isnotnull(Issue Date#13), isnotnull(Violation Time#19), (substring(cast(Issue Date#13 as string), 1, 4) = 2017), NOT (Violation Time#19 = nan), NOT (Violation Time#19 = Nan), NOT (CASE WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 0) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 400)) THEN Dawn WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 400) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 800)) THEN Early Mornig WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 800) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 1200)) THEN Morning Rush WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 1200) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 1600)) THEN Afternoon WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 1600) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) < 2000)) THEN Office Time WHEN ((cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) >= 2000) && (cast(CASE WHEN (substring(Violation Time#19, -1, 1) = A) THEN substring(Violation Time#19, 1, 4) WHEN ((substring(Violation Time#19, -1, 1) = P) && (substring(Violation Time#19, 1, 2) = 12)) THEN concat(00, substring(Violation Time#19, 3, 2)) ELSE concat(cast(cast((cast(substring(Violation Time#19, 1, 2) as double) + 12.0) as int) as string), substring(Violation Time#19, 3, 2)) END as int) <= 2400)) THEN Late Night ELSE unknown END = unknown)]\n               +- InMemoryRelation [Summons Number#10L, Plate ID#11, Registration State#12, Issue Date#13, Violation Code#14, Vehicle Body Type#15, Vehicle Make#16, Violation Precinct#17, Issuer Precinct#18, Violation Time#19], StorageLevel(disk, memory, deserialized, 1 replicas)\n                     +- *(1) FileScan csv [Summons Number#10L,Plate ID#11,Registration State#12,Issue Date#13,Violation Code#14,Vehicle Body Type#15,Vehicle Make#16,Violation Precinct#17,Issuer Precinct#18,Violation Time#19] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/common_folder/nyc_parking/Parking_Violations_Issued_-_Fisca..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Summons Number:bigint,Plate ID:string,Registration State:string,Issue Date:timestamp,Viola...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:136)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.GeneratedMethodAccessor70.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.needToCopyObjectsBeforeShuffle(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:303)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 39 more\n"
     ]
    }
   ],
   "source": [
    "# Calculating count of tickets after grouping the dataframe by \"Registration State\" and sorting in descending order\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "from pyspark.sql.functions import col, asc, desc\n",
    "parking2017DF.groupBy(\"Registration State\").agg(expr(\"count(`Summons Number`)\")\\\n",
    "                                                .alias(\"total_tickets\")).sort(desc(\"total_tickets\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*New York has got the most no of cars with parking tickets.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Displaying the distinct values of such States and total no of States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Registration State|\n",
      "+------------------+\n",
      "|                99|\n",
      "|                AB|\n",
      "|                AK|\n",
      "|                AL|\n",
      "|                AR|\n",
      "|                AZ|\n",
      "|                BC|\n",
      "|                CA|\n",
      "|                CO|\n",
      "|                CT|\n",
      "|                DC|\n",
      "|                DE|\n",
      "|                DP|\n",
      "|                FL|\n",
      "|                FO|\n",
      "|                GA|\n",
      "|                GV|\n",
      "|                HI|\n",
      "|                IA|\n",
      "|                ID|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------------------------+\n",
      "|count(DISTINCT Registration State)|\n",
      "+----------------------------------+\n",
      "|                                65|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT distinct `Registration State` FROM parking_2017\").sort(asc(\"Registration State\")).show()\n",
    "parking2017DF.select(countDistinct(\"Registration State\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now there is a numeric entry '99' in the column, which should be corrected and as per business requirement replaced with the state having the maximum entries - in this case which is `NY` *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|count(DISTINCT Registration State)|\n",
      "+----------------------------------+\n",
      "|                                64|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "parking2017DF = parking2017DF.withColumn(\"Registration State\", \\\n",
    "                                       when(parking2017DF[\"Registration State\"] == 99, 'NY').\\\n",
    "                                       otherwise(parking2017DF[\"Registration State\"]))\n",
    "\n",
    "parking2017DF.select(countDistinct(\"Registration State\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After replacing '99' by `NY`, total no of unique states = `64`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "parking2017DF.registerTempTable(\"parking_2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deleting null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropNullDF = parking2017DF.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|count(DISTINCT Summons Number)|\n",
      "+------------------------------+\n",
      "|                       5431918|\n",
      "+------------------------------+\n",
      "\n",
      "+------------------------------+\n",
      "|count(DISTINCT Summons Number)|\n",
      "+------------------------------+\n",
      "|                       5431918|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.functions import countDistinct\n",
    "parking2017DF.select(countDistinct(\"Summons Number\")).show()\n",
    "dropNullDF.select(countDistinct(\"Summons Number\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There are no null values in the dataframe.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How often does each violation code occur? Display the frequency of the top five violation codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grouping the data based on Violation Code and displaying the top 5 occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|Violation Code|total_violations|\n",
      "+--------------+----------------+\n",
      "|            21|          768087|\n",
      "|            36|          662765|\n",
      "|            38|          542079|\n",
      "|            14|          476664|\n",
      "|            20|          319646|\n",
      "+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "parking2017DF.groupBy(\"Violation Code\").agg(expr(\"count(`Summons Number`)\").\\\n",
    "                                           alias(\"total_violations\")).sort(desc(\"total_violations\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How often does each 'vehicle body type' get a parking ticket? How about the 'vehicle make'? (Hint: Find the top 5 for both.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grouping the data based on Vehicle Body Type and Vehicle Make and displaying the top 5 results for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n",
      "|Vehicle Body Type|total_violations|\n",
      "+-----------------+----------------+\n",
      "|             SUBN|         1883954|\n",
      "|             4DSD|         1547312|\n",
      "|              VAN|          724029|\n",
      "|             DELV|          358984|\n",
      "|              SDN|          194197|\n",
      "+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+----------------+\n",
      "|Vehicle Make|total_violations|\n",
      "+------------+----------------+\n",
      "|        FORD|          636844|\n",
      "|       TOYOT|          605291|\n",
      "|       HONDA|          538884|\n",
      "|       NISSA|          462017|\n",
      "|       CHEVR|          356032|\n",
      "+------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parking2017DF.groupBy(\"Vehicle Body Type\").agg(expr(\"count(`Summons Number`)\").\\\n",
    "                                           alias(\"total_violations\")).sort(desc(\"total_violations\")).show(5)\n",
    "                                           \n",
    "parking2017DF.groupBy(\"Vehicle Make\").agg(expr(\"count(`Summons Number`)\").\\\n",
    "                                           alias(\"total_violations\")).sort(desc(\"total_violations\")).show(5)                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. A precinct is a police station that has a certain zone of the city under its command. Find the (5 highest) frequencies of tickets for each of the following:\n",
    "    1.'Violation Precinct' (This is the precinct of the zone where the violation occurred). Using this, can you draw any \n",
    "        insights for parking violations in any specific areas of the city?\n",
    "    2.'Issuer Precinct' (This is the precinct that issued the ticket.) \n",
    "        Here, you would have noticed that the dataframe has the'Violating Precinct' or 'Issuing Precinct' as '0'. These are \n",
    "        erroneous entries. Hence, you need to provide the records for five correct precincts.\n",
    "        (Hint: Print the top six entries after sorting.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grouping the data based on 'Violation Precinct' & 'Issuer Precinct' and displaying the top 6 occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+\n",
      "|Violation Precinct|total_violations|\n",
      "+------------------+----------------+\n",
      "|                 0|          925596|\n",
      "|                19|          274445|\n",
      "|                14|          203553|\n",
      "|                 1|          174702|\n",
      "|                18|          169131|\n",
      "|               114|          147444|\n",
      "+------------------+----------------+\n",
      "only showing top 6 rows\n",
      "\n",
      "+---------------+----------------+\n",
      "|Issuer Precinct|total_violations|\n",
      "+---------------+----------------+\n",
      "|              0|         1078406|\n",
      "|             19|          266961|\n",
      "|             14|          200495|\n",
      "|              1|          168740|\n",
      "|             18|          162994|\n",
      "|            114|          144054|\n",
      "+---------------+----------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parking2017DF.groupBy(\"Violation Precinct\").agg(expr(\"count(`Summons Number`)\").\\\n",
    "                                           alias(\"total_violations\")).sort(desc(\"total_violations\")).show(6)\n",
    "                                           \n",
    "parking2017DF.groupBy(\"Issuer Precinct\").agg(expr(\"count(`Summons Number`)\").\\\n",
    "                                           alias(\"total_violations\")).sort(desc(\"total_violations\")).show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Find the violation code frequencies for three precincts that have issued the most number of tickets. Do these precinct zones have an exceptionally high frequency of certain violation codes? Are these codes common across precincts? \n",
    "(Hint: In the SQL view, use the 'where' attribute to filter among three precincts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Displaying the three precincts that have issued the most number of tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|Issuer Precinct|total_violation|\n",
      "+---------------+---------------+\n",
      "|             19|         266961|\n",
      "|             14|         200495|\n",
      "|              1|         168740|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT `Issuer Precinct`, COUNT(*) AS total_violation \\\n",
    "FROM parking_2017 \\\n",
    "WHERE `Issuer Precinct` != 0 \\\n",
    "group by `Issuer Precinct` \\\n",
    "order by total_violation desc limit 3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Displaying the 5 most commonly occuring violation codes from the three precincts that have issued the most number of tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+----+\n",
      "|Issuer Precinct|Violation Code|total_violation|RANK|\n",
      "+---------------+--------------+---------------+----+\n",
      "|              1|            14|          38354|   1|\n",
      "|              1|            16|          19081|   2|\n",
      "|              1|            20|          15408|   3|\n",
      "|              1|            46|          12745|   4|\n",
      "|              1|            38|           8535|   5|\n",
      "|             19|            46|          48445|   1|\n",
      "|             19|            38|          36386|   2|\n",
      "|             19|            37|          36056|   3|\n",
      "|             19|            14|          29797|   4|\n",
      "|             19|            21|          28415|   5|\n",
      "|             14|            14|          45036|   1|\n",
      "|             14|            69|          30464|   2|\n",
      "|             14|            31|          22555|   3|\n",
      "|             14|            47|          18364|   4|\n",
      "|             14|            42|          10027|   5|\n",
      "+---------------+--------------+---------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `Issuer Precinct`, `Violation Code`, total_violation, RANK  from \\\n",
    "          (select *, row_number() over (PARTITION by `Issuer Precinct` order by total_violation desc) as RANK from \\\n",
    "            (SELECT b.`Violation Code`, b.`Issuer Precinct`, COUNT(*) AS total_violation \\\n",
    "            FROM (SELECT `Issuer Precinct`, COUNT(*) AS total_violation \\\n",
    "            FROM parking_2017 \\\n",
    "            WHERE `Issuer Precinct` != 0 \\\n",
    "            group by `Issuer Precinct` \\\n",
    "            order by total_violation desc limit 3) as a \\\n",
    "            inner join parking_2017 as b on a.`Issuer Precinct` = b.`Issuer Precinct` \\\n",
    "            group by b.`Violation Code`,b.`Issuer Precinct`) as c) where RANK < 6\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The most commonly occuring violation code for different Precincts are -\n",
    " - Precinct 1 -> 14\n",
    " - Precinct 14 -> 14 \n",
    " - Precinct 19 -> 46\n",
    " \n",
    "*The violation code - 14 seems to be a common factor in the top 5 most occuring violation codes for the 3 precincts* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Find out the properties of parking violations across different times of the day:\n",
    " - Find a way to deal with missing values, if any.\n",
    "(Hint: Check for the null values using 'isNull' under the SQL. Also, to remove the null values, check the 'dropna' command in the API documentation.)\n",
    "\n",
    " - The Violation Time field is specified in a strange format. Find a way to make this a time attribute that you can use to divide into groups.\n",
    "\n",
    " - Divide 24 hours into six equal discrete bins of time. Choose the intervals as you see fit. For each of these groups, find the three most commonly occurring violations.\n",
    "(Hint: Use the CASE-WHEN in SQL view to segregate into bins. To find the most commonly occurring violations, you can use an approach similar to the one mentioned in the hint for question 4.)\n",
    "\n",
    " - Now, try another direction. For the three most commonly occurring violation codes, find the most common time of the day (in terms of the bins from the previous part)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Displaying no of occurences of the following in the dataframe --\n",
    "\n",
    " - NULL\n",
    " - BLANK\n",
    " - Nan/nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---------------------+\n",
      "|count(DISTINCT Violation Time)|count(Violation Time)|\n",
      "+------------------------------+---------------------+\n",
      "|                             0|                    0|\n",
      "+------------------------------+---------------------+\n",
      "\n",
      "+------------------------------+---------------------+\n",
      "|count(DISTINCT Violation Time)|count(Violation Time)|\n",
      "+------------------------------+---------------------+\n",
      "|                             0|                    0|\n",
      "+------------------------------+---------------------+\n",
      "\n",
      "+------------------------------+---------------------+\n",
      "|count(DISTINCT Violation Time)|count(Violation Time)|\n",
      "+------------------------------+---------------------+\n",
      "|                             1|                   16|\n",
      "+------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropNullDF1 = parking2017DF.filter(col(\"Violation Time\").isNull())\n",
    "dropNullDF2 = parking2017DF.where(\"`Violation Time` == ''\")\n",
    "dropNullDF3 = parking2017DF.where((col(\"Violation Time\") == \"nan\") | (col(\"Violation Time\") == \"Nan\"))\n",
    "dropNullDF1.select(countDistinct(\"Violation Time\"), count(\"Violation Time\")).show()\n",
    "dropNullDF2.select(countDistinct(\"Violation Time\"), count(\"Violation Time\")).show()\n",
    "dropNullDF3.select(countDistinct(\"Violation Time\"), count(\"Violation Time\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There seems to be 16 records in the dataframe that have the value 'Nan' or 'nan' in the column 'Violation Time'. We will be dropping these records from the dataframe.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---------------------+\n",
      "|count(DISTINCT Violation Time)|count(Violation Time)|\n",
      "+------------------------------+---------------------+\n",
      "|                          1625|              5431902|\n",
      "+------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parking2017DF = parking2017DF.filter((col(\"Violation Time\") != \"nan\") & (col(\"Violation Time\") != \"Nan\"))\n",
    "\n",
    "parking2017DF.select(countDistinct(\"Violation Time\"), count(\"Violation Time\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After dropping 'Nan/nan' records from the dataframe, there are 5431902 records left.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "parking2017DF.registerTempTable(\"parking_2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Violation Time field is specified in a strange format (Eg. 1120A). We are transforming it as per the below logic in a new column Violation Time Fixed:\n",
    "\n",
    " - If the `Violation Time` value ends with 'A', then we just take the 1st 4 digits and ignore the alphabet 'A'. Eg. 1120A -> 1120.\n",
    " - If the `Violation Time` value ends with 'P', and the first 2 digits are '12' then we concatenate '00' with the 3rd and 4th digit and ignore the alphabet 'P'. Eg. 1220P -> 0020.\n",
    " - For all other types of occurences, ie. if the `Violation Time` value ends with 'P', and the first 2 digits aren't '12' then we add '12' to the first 2 digits and concatenate it with the 3rd and 4th digit and ignore the alphabet 'P'. Eg. 0852P -> 2052."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+--------+--------------------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|Issue_Yr|Violation Time Fixed|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+--------+--------------------+\n",
      "|    8478629828| 66623ME|                NY|2017-06-14 00:00:00|            47|             REFG|       MITSU|                14|             14|         1120A|    2017|                1120|\n",
      "|    5096917368| FZD8593|                NY|2017-06-13 00:00:00|             7|             SUBN|       ME/BE|                 0|              0|         0852P|    2017|                2052|\n",
      "|    1407740258| 2513JMG|                NY|2017-01-11 00:00:00|            78|             DELV|       FRUEH|               106|            106|         0015A|    2017|                0015|\n",
      "|    1413656420|T672371C|                NY|2017-02-04 00:00:00|            40|             TAXI|       TOYOT|                73|             73|         0525A|    2017|                0525|\n",
      "|    8480309064| 51771JW|                NY|2017-01-26 00:00:00|            64|              VAN|       INTER|                17|             17|         0256P|    2017|                1456|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parking2017DF = spark.sql(\"SELECT *, \\\n",
    "                        CASE WHEN substr(`Violation Time`, -1,1) == 'A' \\\n",
    "                            THEN substr(`Violation Time`, 1,4) \\\n",
    "                            WHEN substr(`Violation Time`, -1,1) == 'P' AND substr(`Violation Time`, 1,2) == '12'\\\n",
    "                            THEN CONCAT('00',substr(`Violation Time`, 3,2)) \\\n",
    "                            ELSE CONCAT(CAST((substr(`Violation Time`, 1,2) + 12) as int),substr(`Violation Time`, 3,2)) \\\n",
    "                            END AS `Violation Time Fixed` \\\n",
    "FROM parking_2017\")\n",
    "parking2017DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "parking2017DF.registerTempTable(\"parking_2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dividing the 24 hours into six equal discrete bins of time based on the new column Violation Time Fixed:\n",
    " - Between 0 and 400 --> Dawn\n",
    " - Between 400 and 800 --> Early Mornig\n",
    " - Between 800 and 1200 --> Morning Rush\n",
    " - Between 1200 and 1600 --> Afternoon\n",
    " - Between 1600 and 2000 --> Office Time\n",
    " - Between 2000 and 2400 --> Late Night\n",
    " - For everything else --> unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+--------+--------------------+-------------------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|Issue_Yr|Violation Time Fixed|Violation Time Span|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+--------+--------------------+-------------------+\n",
      "|    8478629828| 66623ME|                NY|2017-06-14 00:00:00|            47|             REFG|       MITSU|                14|             14|         1120A|    2017|                1120|       Morning Rush|\n",
      "|    5096917368| FZD8593|                NY|2017-06-13 00:00:00|             7|             SUBN|       ME/BE|                 0|              0|         0852P|    2017|                2052|         Late Night|\n",
      "|    1407740258| 2513JMG|                NY|2017-01-11 00:00:00|            78|             DELV|       FRUEH|               106|            106|         0015A|    2017|                0015|               Dawn|\n",
      "|    1413656420|T672371C|                NY|2017-02-04 00:00:00|            40|             TAXI|       TOYOT|                73|             73|         0525A|    2017|                0525|       Early Mornig|\n",
      "|    8480309064| 51771JW|                NY|2017-01-26 00:00:00|            64|              VAN|       INTER|                17|             17|         0256P|    2017|                1456|          Afternoon|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+--------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parking2017DF = spark.sql(\"SELECT *, \\\n",
    "                        CASE WHEN `Violation Time Fixed` >= 0 and `Violation Time Fixed` < 400\\\n",
    "                            THEN 'Dawn' \\\n",
    "                            WHEN `Violation Time Fixed` >= 400 and `Violation Time Fixed` < 800\\\n",
    "                            THEN 'Early Mornig' \\\n",
    "                            WHEN `Violation Time Fixed` >= 800 and `Violation Time Fixed` < 1200\\\n",
    "                            THEN 'Morning Rush' \\\n",
    "                            WHEN `Violation Time Fixed` >= 1200 and `Violation Time Fixed` < 1600\\\n",
    "                            THEN 'Afternoon' \\\n",
    "                            WHEN `Violation Time Fixed` >= 1600 and `Violation Time Fixed` < 2000\\\n",
    "                            THEN 'Office Time' \\\n",
    "                            WHEN `Violation Time Fixed` >= 2000 and `Violation Time Fixed` <= 2400\\\n",
    "                            THEN 'Late Night' \\\n",
    "                            ELSE 'unknown' \\\n",
    "                            END AS `Violation Time Span` \\\n",
    "FROM parking_2017\")\n",
    "parking2017DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "parking2017DF.registerTempTable(\"parking_2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the count of records where Violation Time Span = unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-------------------+\n",
      "|Violation Time|Violation Time Fixed|Violation Time Span|\n",
      "+--------------+--------------------+-------------------+\n",
      "|         6815P|                8015|            unknown|\n",
      "|         110+A|                110+|            unknown|\n",
      "|         093+A|                093+|            unknown|\n",
      "|         8715P|                9915|            unknown|\n",
      "|         5402P|                6602|            unknown|\n",
      "+--------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `Violation Time`,`Violation Time Fixed`, `Violation Time Span`  \\\n",
    "from parking_2017 where `Violation Time Span` == 'unknown'\").show(5)\n",
    "\n",
    "spark.sql(\"select count(*) from parking_2017 where `Violation Time Span` == 'unknown'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are 73 records where Violation Time Span = unknown. These are essentially faulty records and need to be removed. For eg, Violation Time values for some of these records are `6815P`, `110+A`, `093+A`, etc. Thus these records could not be binned into Violation Time Span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+--------+--------------------+-------------------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|Issue_Yr|Violation Time Fixed|Violation Time Span|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+--------+--------------------+-------------------+\n",
      "|    8478629828| 66623ME|                NY|2017-06-14 00:00:00|            47|             REFG|       MITSU|                14|             14|         1120A|    2017|                1120|       Morning Rush|\n",
      "|    5096917368| FZD8593|                NY|2017-06-13 00:00:00|             7|             SUBN|       ME/BE|                 0|              0|         0852P|    2017|                2052|         Late Night|\n",
      "|    1407740258| 2513JMG|                NY|2017-01-11 00:00:00|            78|             DELV|       FRUEH|               106|            106|         0015A|    2017|                0015|               Dawn|\n",
      "|    1413656420|T672371C|                NY|2017-02-04 00:00:00|            40|             TAXI|       TOYOT|                73|             73|         0525A|    2017|                0525|       Early Mornig|\n",
      "|    8480309064| 51771JW|                NY|2017-01-26 00:00:00|            64|              VAN|       INTER|                17|             17|         0256P|    2017|                1456|          Afternoon|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+--------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------------------------+--------------------------+\n",
      "|count(DISTINCT Violation Time Span)|count(Violation Time Span)|\n",
      "+-----------------------------------+--------------------------+\n",
      "|                                  6|                   5431829|\n",
      "+-----------------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parking2017DF = parking2017DF.filter(col(\"`Violation Time Span`\") != \"unknown\")\n",
    "parking2017DF.show(5)\n",
    "parking2017DF.select(countDistinct(\"Violation Time Span\"), count(\"Violation Time Span\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After removal of faulty records, the no of rows in the dataframe  = 5431829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "parking2017DF.registerTempTable(\"parking_2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Displaying the 3 most commonly occuring violation codes for each of the 6 different Violation Time Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+----------------+\n",
      "|Violation Time Span|Violation Code|no_of_violations|\n",
      "+-------------------+--------------+----------------+\n",
      "|               Dawn|            21|          107065|\n",
      "|               Dawn|            36|          101991|\n",
      "|               Dawn|            38|           56204|\n",
      "|        Office Time|            38|          102855|\n",
      "|        Office Time|            14|           75902|\n",
      "|        Office Time|            37|           70345|\n",
      "|          Afternoon|            38|          184829|\n",
      "|          Afternoon|            36|          184293|\n",
      "|          Afternoon|            37|          130692|\n",
      "|       Early Mornig|            14|           74113|\n",
      "|       Early Mornig|            40|           60652|\n",
      "|       Early Mornig|            21|           57894|\n",
      "|         Late Night|             7|           26293|\n",
      "|         Late Night|            40|           22336|\n",
      "|         Late Night|            14|           21045|\n",
      "|       Morning Rush|            21|          598062|\n",
      "|       Morning Rush|            36|          348165|\n",
      "|       Morning Rush|            38|          176570|\n",
      "+-------------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `Violation Time Span`, `Violation Code`, no_of_violations from \\\n",
    "         (select *, row_number() over (PARTITION by `Violation Time Span` order by no_of_violations desc) as num from \\\n",
    "         (select `Violation Time Span`, `Violation Code`, count(*) as no_of_violations \\\n",
    "         from parking_2017 group by `Violation Time Span`, `Violation Code`) as a) \\\n",
    "         where num < 4\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Displaying the most common Violation Time Span of the day for the three most commonly occurring violation codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+---------------+\n",
      "|Violation Code|Violation Time Span|total_violation|\n",
      "+--------------+-------------------+---------------+\n",
      "|            38|          Afternoon|         184829|\n",
      "|            21|       Morning Rush|         598062|\n",
      "|            36|       Morning Rush|         348165|\n",
      "+--------------+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select`Violation Code`,`Violation Time Span`,total_violation from \\\n",
    "          (select *, row_number() over (PARTITION by `Violation Code` order by total_violation desc) as num from \\\n",
    "            (SELECT b.`Violation Code`, b.`Violation Time Span`, COUNT(*) AS total_violation \\\n",
    "            FROM (SELECT `Violation Code`, COUNT(*) AS total_violation \\\n",
    "            FROM parking_2017 \\\n",
    "            group by `Violation Code` \\\n",
    "            order by total_violation desc limit 3) as a \\\n",
    "            inner join parking_2017 as b on a.`Violation Code` = b.`Violation Code` \\\n",
    "            group by b.`Violation Code`,b.`Violation Time Span`) as c) where num < 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Lets try and find some seasonality in this data:\n",
    "\n",
    " - First, divide the year into a certain number of seasons, and find the frequencies of tickets for each season. (Hint: Use Issue Date to segregate into seasons.)\n",
    "\n",
    " - Then, find the three most common violations for each of these seasons. (Hint: You can use an approach similar to the one mentioned in the hint for question 4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "parking2017DF.registerTempTable(\"parking_2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dividing the year into following number of seasons based on the column Issue Date into a new column  Issue Season:\n",
    " - Month between 3 and 5 --> Spring\n",
    " - Month between 6 and 8 --> Summer\n",
    " - Month between 9 and 11 --> Fall\n",
    " - Else --> Winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+--------+--------------------+-------------------+------------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|Issue_Yr|Violation Time Fixed|Violation Time Span|Issue Season|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+--------+--------------------+-------------------+------------+\n",
      "|    8478629828| 66623ME|                NY|2017-06-14 00:00:00|            47|             REFG|       MITSU|                14|             14|         1120A|    2017|                1120|       Morning Rush|      Summer|\n",
      "|    5096917368| FZD8593|                NY|2017-06-13 00:00:00|             7|             SUBN|       ME/BE|                 0|              0|         0852P|    2017|                2052|         Late Night|      Summer|\n",
      "|    1407740258| 2513JMG|                NY|2017-01-11 00:00:00|            78|             DELV|       FRUEH|               106|            106|         0015A|    2017|                0015|               Dawn|      Winter|\n",
      "|    1413656420|T672371C|                NY|2017-02-04 00:00:00|            40|             TAXI|       TOYOT|                73|             73|         0525A|    2017|                0525|       Early Mornig|      Winter|\n",
      "|    8480309064| 51771JW|                NY|2017-01-26 00:00:00|            64|              VAN|       INTER|                17|             17|         0256P|    2017|                1456|          Afternoon|      Winter|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+--------+--------------------+-------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parking2017DF = spark.sql(\"SELECT *, \\\n",
    "CASE WHEN substr(`Issue Date`,6,2) between 3 and 5 \\\n",
    "    THEN 'Spring' \\\n",
    "    WHEN substr(`Issue Date`,6,2) between 6 and 8 \\\n",
    "    THEN 'Summer' \\\n",
    "    WHEN substr(`Issue Date`,6,2) between 9 and 11 \\\n",
    "    THEN 'Fall' \\\n",
    "    ELSE 'Winter' \\\n",
    "    END AS `Issue Season` \\\n",
    "                        FROM parking_2017\")\n",
    "parking2017DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "parking2017DF.registerTempTable(\"parking_2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Displaying the 3 most commonly occuring violation codes for each of the 4 seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+----------------+\n",
      "|Issue Season|Violation Code|no_of_violations|\n",
      "+------------+--------------+----------------+\n",
      "|      Spring|            21|          402401|\n",
      "|      Spring|            36|          344834|\n",
      "|      Spring|            38|          271167|\n",
      "|      Summer|            21|          127345|\n",
      "|      Summer|            36|           96663|\n",
      "|      Summer|            38|           83518|\n",
      "|        Fall|            46|             231|\n",
      "|        Fall|            21|             128|\n",
      "|        Fall|            40|             116|\n",
      "|      Winter|            21|          238180|\n",
      "|      Winter|            36|          221268|\n",
      "|      Winter|            38|          187385|\n",
      "+------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `Issue Season`, `Violation Code`, no_of_violations from \\\n",
    "         (select *, row_number() over (PARTITION by `Issue Season` order by no_of_violations desc) as num from \\\n",
    "         (select `Issue Season`, `Violation Code`, count(*) as no_of_violations \\\n",
    "         from parking_2017 group by `Issue Season`, `Violation Code`) as a) \\\n",
    "         where num < 4\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. The fines collected from all the instances of parking violation constitute a source of revenue for the NYC Police Department. Lets take an example of estimating this for the three most commonly occurring codes:\n",
    " - Find the total occurrences of the three most common violation codes.\n",
    " - Then, visit the website: http://www1.nyc.gov/site/finance/vehicles/services-violation-codes.page . It lists the fines associated with different violation codes. Theyre divided into two categories: one for the highest-density locations in the city and the other for the rest of the city. For the sake of simplicity, take the average of the two.\n",
    " - Using this information, find the total amount collected for the three violation codes with the maximum tickets. State the code that has the highest total collection.\n",
    " - What can you intuitively infer from these findings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding the total occurrences of the three most common violation codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|Violation Code|no_of_violations|\n",
      "+--------------+----------------+\n",
      "|            21|          768054|\n",
      "|            36|          662765|\n",
      "|            38|          542078|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top3violationcodes = spark.sql(\"select `Violation Code`, count(*) as no_of_violations \\\n",
    "         from parking_2017 group by `Violation Code` order by no_of_violations desc limit 3\")\n",
    "top3violationcodes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register DataFrame as temp table\n",
    "top3violationcodes.registerTempTable(\"violation_codes_df\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing the fines associated with the 3 different violation codes : one for the highest-density locations in the city and the other for the rest of the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+---------------------+-----------------+\n",
      "|Violation Code|no_of_violations|High_Density_Loc_Fine|Rest_of_City_Fine|\n",
      "+--------------+----------------+---------------------+-----------------+\n",
      "|            21|          768054|                   65|               45|\n",
      "|            36|          662765|                   50|               50|\n",
      "|            38|          542078|                   65|               35|\n",
      "+--------------+----------------+---------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top3violationcodes = spark.sql(\"SELECT *, \\\n",
    "                                CASE WHEN `Violation Code` == 21 \\\n",
    "                                THEN  65 \\\n",
    "                                WHEN `Violation Code` == 36 \\\n",
    "                                THEN 50 \\\n",
    "                                ELSE 65 \\\n",
    "                                END AS `High_Density_Loc_Fine`, \\\n",
    "                                CASE WHEN `Violation Code` == 21 \\\n",
    "                                THEN  45 \\\n",
    "                                WHEN `Violation Code` == 36 \\\n",
    "                                THEN 50 \\\n",
    "                                ELSE 35 \\\n",
    "                                END AS `Rest_of_City_Fine` \\\n",
    "                                FROM violation_codes_df\")\n",
    "top3violationcodes.show(5)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "top3violationcodes.registerTempTable(\"violation_codes_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Taking the average of the 2 categories of fines for each of the 3 different violation codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+---------------------+-----------------+------------+\n",
      "|Violation Code|no_of_violations|High_Density_Loc_Fine|Rest_of_City_Fine|Average Fine|\n",
      "+--------------+----------------+---------------------+-----------------+------------+\n",
      "|            21|          768054|                   65|               45|          55|\n",
      "|            36|          662765|                   50|               50|          50|\n",
      "|            38|          542078|                   65|               35|          50|\n",
      "+--------------+----------------+---------------------+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top3violationcodes = spark.sql(\"SELECT *, \\\n",
    "                                 cast((`High_Density_Loc_Fine` + `Rest_of_City_Fine`)/2 as int) AS `Average Fine` \\\n",
    "                                FROM violation_codes_df\")\n",
    "top3violationcodes.show(5)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "top3violationcodes.registerTempTable(\"violation_codes_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding the total amount collected for the three violation codes with the maximum tickets by multiplying the average fine per violation to the total occurences of each violation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+---------------------+-----------------+------------+----------+\n",
      "|Violation Code|no_of_violations|High_Density_Loc_Fine|Rest_of_City_Fine|Average Fine|Total Fine|\n",
      "+--------------+----------------+---------------------+-----------------+------------+----------+\n",
      "|            21|          768054|                   65|               45|          55|  42242970|\n",
      "|            36|          662765|                   50|               50|          50|  33138250|\n",
      "|            38|          542078|                   65|               35|          50|  27103900|\n",
      "+--------------+----------------+---------------------+-----------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT *, `Average Fine` * no_of_violations as `Total Fine` FROM violation_codes_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Violation Code - 21 has the highest total collection in terms of fine*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping the SparkContext Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
